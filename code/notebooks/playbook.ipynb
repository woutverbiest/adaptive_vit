{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118bfa88-e2a7-4361-a0fe-a7dc7ac2a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import mmcv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# decode heads (atm_head, tpn_atm_head)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "import math\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f90c01-c978-4c24-9bd2-421a013cfff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_segmentor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoint_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/checkpoint/COCOstuff_49.9.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# cpu model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minit_segmentor\u001b[49m(config_file, checkpoint_file, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Setting the r values\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model.backbone.device = 'cpu'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m r_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.995\u001b[39m, \u001b[38;5;241m0.93\u001b[39m, \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mlayers))\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_segmentor' is not defined"
     ]
    }
   ],
   "source": [
    "config_file = '/project_ghent/woverbie/code/configs/config.py'\n",
    "# checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "checkpoint_file = '../model/checkpoint/COCOstuff_49.9.pth'\n",
    "# model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "# cpu model\n",
    "model = init_segmentor(config_file, checkpoint_file, device='cpu')\n",
    "\n",
    "# Setting the r values\n",
    "\n",
    "# model.backbone.device = 'cpu'\n",
    "\n",
    "r_values = np.linspace(0.995, 0.93, len(model.backbone.layers)).tolist()\n",
    "for i, block in enumerate(model.backbone.layers):\n",
    "    # Assuming the transformer blocks are in backbone.layers\n",
    "    model.backbone.layers[i].r = r_values[i]\n",
    "    # model.backbone.layers[i].device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fda1a1-0615-4c92-b72b-7430652cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/project_ghent/woverbie/code/configs/config.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "model_og = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "for i, block in enumerate(model_og.backbone.layers):\n",
    "    model_og.backbone.layers[i].r = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4053c0-51e0-4516-b397-5d59e94e21e9",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706081b-5bde-438b-a9c8-229d22165d2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# imgs = sorted(glob('../dataset/camera_lidar/20180810_150607/camera/cam_front_center/*.png'))\n",
    "\n",
    "\n",
    "# result = inference_segmentor(model, imgs[0])\n",
    "\n",
    "# plt.imshow(result[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c8daa-d783-43ee-8388-0d9cea3354cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(image_rgb, result):\n",
    "\n",
    "    base_h, base_w, _ = image_rgb.shape\n",
    "    overlay_h, overlay_w = result.shape\n",
    "    \n",
    "    # Compute the top-left corner for centering the square overlay\n",
    "    x_offset = (base_w - overlay_w) // 2\n",
    "    y_offset = (base_h - overlay_h) // 2\n",
    "    \n",
    "    # Normalize result[0] to range from 0 to 1 for mapping to colors\n",
    "    normalized_square = (result - 1) / 170  # Normalize values to range [0, 1]\n",
    "    \n",
    "    # Use a colormap to assign RGB colors to the normalized square\n",
    "    colored_square = cv2.applyColorMap(np.uint8(normalized_square * 255), cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Set the opacity of the overlay (0.0 means fully transparent, 1.0 means fully opaque)\n",
    "    alpha = 0.5  # 50% opacity\n",
    "    \n",
    "    # Create a copy of the base image to preserve the original\n",
    "    output_image = image_rgb.copy()\n",
    "    \n",
    "    # Perform the blending for each channel (R, G, B)\n",
    "    for c in range(3):  # Assuming the images are RGB\n",
    "        output_image[y_offset:y_offset + overlay_h, x_offset:x_offset + overlay_w, c] = \\\n",
    "            (alpha * colored_square[:, :, c] + (1 - alpha) * image_rgb[y_offset:y_offset + overlay_h, x_offset:x_offset + overlay_w, c])\n",
    "    \n",
    "    \n",
    "    # Display or save the result\n",
    "    plt.imshow(output_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b972253-e749-4a41-8775-01db187e123c",
   "metadata": {},
   "source": [
    "# Merge Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e54b83-7583-4e59-8bac-3da8affb909f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def do_not_merge(x, mode=None):\n",
    "#     return x, None, None, None\n",
    "\n",
    "# def do_not_unmerge(x, unm_idx, src_idx, dst_idx, out_layer):\n",
    "#     return x\n",
    "\n",
    "# def custom_merge(\n",
    "#     metric: torch.Tensor,\n",
    "#     token_history,\n",
    "#     token_history_processed,\n",
    "#     r: float,\n",
    "#     class_token: bool = False,\n",
    "#     distill_token:bool = False\n",
    "# ) -> Tuple[Callable, Callable,]:\n",
    "#     # protected = 0\n",
    "#     # if class_token:\n",
    "#     #     protected += 1\n",
    "#     # if distill_token:\n",
    "#     #     protected += 1\n",
    "\n",
    "\n",
    "#     # t = metric.shape[1]\n",
    "\n",
    "#     if r >= 1:\n",
    "#         return do_not_merge, do_not_unmerge\n",
    "        \n",
    "#     if token_history.dim() == 0:\n",
    "#         return do_not_merge, do_not_unmerge\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # metric = metric / metric.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         # there is no history left so we can not merge yet.\n",
    "#         if token_history is None:\n",
    "#             return do_not_merge, do_not_unmerge\n",
    "        \n",
    "#         a = metric # the tokens in the current set\n",
    "#         b = token_history # the history\n",
    "        \n",
    "#         dot_product = a @ b.transpose(-1, -2)\n",
    "\n",
    "#         a_norm = a.norm(dim=-1, keepdim=True)  # Shape: (1, x, 1)\n",
    "#         b_norm = b.norm(dim=-1, keepdim=True)  # Shape: (1, x, 1)\n",
    "\n",
    "#         norm_product = a_norm @ b_norm.transpose(-1, -2)\n",
    "\n",
    "#         epsilon = 1e-8\n",
    "#         norm_product = norm_product + epsilon\n",
    "        \n",
    "#         cosine_similarity = dot_product / norm_product\n",
    "\n",
    "#         node_max, node_idx = cosine_similarity.max(dim=-1)\n",
    "\n",
    "#         mask = node_max > r\n",
    "\n",
    "#         if torch.any(mask):\n",
    "            \n",
    "#             src_idx = torch.where(mask)[1]\n",
    "#             src_idx = src_idx.unsqueeze(0).unsqueeze(-1)\n",
    "            \n",
    "#             dst_idx = node_idx[..., None].gather(dim=-2, index=src_idx)\n",
    "\n",
    "#         else:\n",
    "#             src_idx = None\n",
    "#             dst_idx = None\n",
    "\n",
    "#         unm_idx = torch.where(~mask)[1]\n",
    "#         unm_idx = unm_idx.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    \n",
    "\n",
    "#     def merge(x: torch.Tensor, mode=\"mean\") -> torch.Tensor:\n",
    "#         # what should a merge actually do?\n",
    "#         # it should return the unmatched\n",
    "\n",
    "#         if(src_idx is not None):\n",
    "#             src = x # the source of tokens is just x\n",
    "    \n",
    "#             n, t1, c = src.shape\n",
    "    \n",
    "#             _, t2, _ = unm_idx.shape\n",
    "            \n",
    "#             unmatched_tokens = src.gather(dim=-2, index=unm_idx.expand(n, t2, c))\n",
    "#         else:\n",
    "#             unmatched_tokens = x\n",
    "#         return unmatched_tokens, unm_idx, src_idx, dst_idx\n",
    "\n",
    "#     def unmerge(x: torch.Tensor, unm_idx: torch.Tensor, src_idx: torch.Tensor, dst_idx: torch.Tensor, out_layer):\n",
    "#         if src_idx is not None:\n",
    "\n",
    "            \n",
    "#             # `unm` is the original tensor x\n",
    "#             unm = x\n",
    "#             # `dst` is the tensor containing tokens to be inserted, taken from token_history\n",
    "#             dst = token_history_processed[out_layer]\n",
    "        \n",
    "#             # Shape parameters\n",
    "#             n, unm_len, c = unm.shape  # Batch size, length of unm, channels\n",
    "#             r = dst_idx.size(1)        # Number of tokens to add from dst\n",
    "#             total_len = unm_len + r    # Combined length of output\n",
    "        \n",
    "#             # Gather the src tokens from `dst` at positions specified by `dst_idx`\n",
    "#             src = torch.index_select(dst, dim=1, index=dst_idx.view(-1)).view(n, r, c)\n",
    "        \n",
    "#             # Concatenate `unm` and `src` along the length dimension and initialize output\n",
    "#             combined = torch.cat((unm, src), dim=1)\n",
    "        \n",
    "#             # Create index tensor for scatter operation\n",
    "#             indices = torch.cat((unm_idx, src_idx), dim=1).expand(n, total_len, c)\n",
    "        \n",
    "#             # Use advanced indexing to rearrange the combined tensor\n",
    "#             out = torch.zeros_like(combined, device=x.device).scatter_(\n",
    "#                 dim=1, index=indices, src=combined\n",
    "#             )\n",
    "        \n",
    "#             return out\n",
    "#         else:\n",
    "#             return x\n",
    "\n",
    "#     return merge, unmerge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831bad81-7ace-48a0-bfdc-7ef8ea0ae5dc",
   "metadata": {},
   "source": [
    "# Custom Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252fc77-c314-4672-987c-6a3513dac2cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Define a custom transformer block that includes the new layer\n",
    "\n",
    "# class CustomTransformerBlock(nn.Module):\n",
    "#     def __init__(self, original_block, layer):\n",
    "#         super(CustomTransformerBlock, self).__init__()\n",
    "\n",
    "#         self.attn = original_block.attn\n",
    "#         self.norm1 = original_block.norm1\n",
    "#         self.ffn = original_block.ffn\n",
    "#         self.norm2 = original_block.norm2\n",
    "#         self.with_cp = original_block.with_cp\n",
    "#         self.layer = layer\n",
    "\n",
    "#         self.unmerge_fn = None\n",
    "#         self.unm_idx = None\n",
    "#         self.src_idx = None\n",
    "#         self.dst_idx = None\n",
    "\n",
    "#         self.token_history = []\n",
    "\n",
    "#         self.token_history_processed = {}\n",
    "\n",
    "#         self.token_history_tensor = torch.zeros([]).to('cuda:0')\n",
    "#         self.token_history_count_tensor = []\n",
    "\n",
    "        \n",
    "\n",
    "#         self.token_history_processed_circular = {}\n",
    "#         self.token_history_circular = torch.zeros((1,2048, 1024), device='cuda:0')\n",
    "#         self.max_len = 2048\n",
    "#         self.token_size = 1024\n",
    "#         self.index_token_history = 0\n",
    "#         self.index_token_history_processed = {}\n",
    "\n",
    "#         self.r = 0.95\n",
    "\n",
    "#         for out_layer in [7, 15, 23]:\n",
    "#             self.token_history_processed.setdefault(out_layer, torch.zeros((1,2048, 1024), device='cuda:0'))\n",
    "#             self.index_token_history_processed[out_layer] = 0\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         def _inner_forward(x):\n",
    "#             # self attention layer\n",
    "#             x = self.attn(self.norm1(x), identity=x)\n",
    "\n",
    "#             # if layer is one that should be reduced\n",
    "#             if self.layer % 3 == 0:\n",
    "#                 _, n_tokens, _ = x.shape\n",
    "\n",
    "#                 # if len(self.token_history_count_tensor)>0:\n",
    "#                 # stacked_history = \n",
    "#                 # stacked_processed_history = \n",
    "\n",
    "#                 merge, unmerge = custom_merge(x, self.token_history_circular , self.token_history_processed, r=self.r)\n",
    "                \n",
    "#                 x, size, unm_idx, src_idx, dst_idx = merge_wavg(merge, x)\n",
    "\n",
    "#                 self.unmerge_fn = unmerge\n",
    "#                 self.unm_idx = unm_idx\n",
    "#                 self.src_idx = src_idx\n",
    "#                 self.dst_idx = dst_idx\n",
    "                \n",
    "#                 _, batch_size, token_size = x.shape\n",
    "#                 end_idx = self.index_token_history + batch_size\n",
    "#                 if end_idx <= self.max_len:\n",
    "#                     # Add tokens without wrapping\n",
    "#                     self.token_history_circular[0, self.index_token_history:end_idx, :] = x[0]\n",
    "#                 else:\n",
    "#                     # Wrap around: split the tokens and write in two parts\n",
    "#                     overflow = end_idx - self.max_len\n",
    "#                     self.token_history_circular[0, self.index_token_history:self.max_len, :] = x[0, :batch_size - overflow, :]\n",
    "#                     self.token_history_circular[0, :overflow, :] = x[0, batch_size - overflow:, :]\n",
    "        \n",
    "#                 # Update the index, wrapping around\n",
    "#                 self.index_token_history = end_idx % self.max_len\n",
    "\n",
    "#             # I add myself to the token merge\n",
    "#             x = self.ffn(self.norm2(x), identity=x)\n",
    "\n",
    "#             return x\n",
    "\n",
    "#         if self.with_cp and x.requires_grad:\n",
    "\n",
    "#             x = cp.checkpoint(_inner_forward, x)\n",
    "#         else:\n",
    "#             x = _inner_forward(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def unmerge(self, x, unm_idx, src_idx, dst_idx, out_layer):\n",
    "\n",
    "#         if self.layer % 3 == 0 or unm_idx is not None:\n",
    "\n",
    "#             reconstructed_tokens = self.unmerge_fn(x, unm_idx, src_idx, dst_idx, out_layer)\n",
    "\n",
    "#             _, batch_size, token_size = x.shape\n",
    "#             end_idx = self.index_token_history_processed[out_layer] + batch_size\n",
    "            \n",
    "#             if end_idx <= self.max_len:\n",
    "#                 # Add tokens without wrapping\n",
    "#                 self.token_history_processed[out_layer][0, self.index_token_history_processed[out_layer]:end_idx, :] = x[0]\n",
    "#             else:\n",
    "#                 # Wrap around: split the tokens and write in two parts\n",
    "#                 overflow = end_idx - self.max_len\n",
    "#                 self.token_history_processed[out_layer][0, self.index_token_history_processed[out_layer]:self.max_len, :] = x[0, :batch_size - overflow, :]\n",
    "#                 self.token_history_processed[out_layer][0, :overflow, :] = x[0, batch_size - overflow:, :]\n",
    "\n",
    "#             # Update the index, wrapping around if needed\n",
    "#             self.index_token_history_processed[out_layer] = end_idx % self.max_len\n",
    "\n",
    "#             return reconstructed_tokens\n",
    "#         else:\n",
    "#             return x\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7934301-5510-4740-814c-e85f68881faa",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34541be-99c4-4b5a-9a3a-907527a3983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    (1,2,3,4,15,16,17, 47): 2, # car 1\n",
    "    # 2: 2, # car 2\n",
    "    # 3: 2, # car 3\n",
    "    # 4: 2, # car 4\n",
    "    (5,6,7,8): 1, # bicycle 1\n",
    "    # 6: 1, # bicycle 2\n",
    "    # 7: 1, # bicycle 3\n",
    "    # 8: 1, # bicycle 4\n",
    "    (9,10,11): 0, # pedestrian 1\n",
    "    # 10: 0, # pedestri3n 2\n",
    "    # 11: 0, # pedestrian 3\n",
    "    (12,13,14,32): 7, # Truck 1\n",
    "    # 13: 7, # Truck 2\n",
    "    # 14: 7, # Truck 3\n",
    "    # (15,16,17): 2, #Small vehicles 1\"\n",
    "    # 16: 2, #Small vehicles 2\"\n",
    "    # 17: 2, #Small vehicles 3\"\n",
    "    (18,19,20): 9, #Traffic signal 1\"\n",
    "    # 19: 9, #Traffic signal 2\"\n",
    "    # 20: 9, #Traffic signal 3\"\n",
    "    (21,22,23): 11,#Traffic sign 1\"\n",
    "    # 22: 11,#Traffic sign 2\"\n",
    "    # 23: 11, #Traffic sign 3\"\n",
    "    (24,25): 7, #Utility vehicle 1\"\n",
    "    # 25: 7 ,#Utility vehicle 2\"\n",
    "    26: 134 ,#Sidebars\"\n",
    "    27: -1,#Speed bumper\"\n",
    "    28: -1,#Curbstone\"\n",
    "    # 29: 137,#Solid line\"\n",
    "    30: 9,#Irrelevant signs\"\n",
    "    31: -1,#Road blocks\"\n",
    "    # 32: 7, #Tractor\"\n",
    "    # 33: 137 ,#Non-drivable street\"\n",
    "    # 34: 137 ,#Zebra crossing\"\n",
    "    35: -1,#Obstacles / trash\"\n",
    "    36: -1,#Poles\"\n",
    "    37: -1,#RD restricted area\"\n",
    "    38: (14,15,16,17,18,19,20,21,22,23),  #Animals\"\n",
    "    39: -1, #Grid structure\"\n",
    "    40: -1, #Signal corpus\"\n",
    "    # 41: 137 ,#Drivable cobblestone\"\n",
    "    42: -1 ,#Electronic traffic\" (digital board contiaining info\n",
    "    # 43: 137 ,#Slow drive area\"\n",
    "    44: (82, 85, 130, 157),#Nature object\"\n",
    "    45: -1, #Parking area\"\n",
    "    # 46: 137,#Sidewalk\":\n",
    "    # 47: 137 ,#Ego car\"\n",
    "    # 48: 137 ,#Painted driv. instr.\"\n",
    "    49: -1 ,#Traffic guide obj.\"\n",
    "    # 50: 137 ,#Dashed line\"\n",
    "    # 51: 137,#RD normal street\"\n",
    "    (29,33,34,41,43,46,48,50,51): 137,\n",
    "    52: (145, 94), #Sky\"\n",
    "    53: (84 ,116, 168, 169, 159, 160, 161, 162, 163, 164, 165), #Buildings\"\n",
    "    54: -1,#Blurred area\"\n",
    "    55: 99,#Rain dirt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e7143-01ff-47a8-bcdd-a2fcf0cb8065",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def similarity_percentage(array1, array2):\n",
    "    # Ensure the arrays have the same shape\n",
    "    if array1.shape != array2.shape:\n",
    "        raise ValueError(\"Arrays must have the same shape for comparison.\")\n",
    "    \n",
    "    # Calculate similarity\n",
    "    identical_elements = np.sum(array1 == array2)\n",
    "    total_elements = array1.size\n",
    "    similarity = (identical_elements / total_elements) * 100\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844b938-ee2e-461a-9d10-1e5501940f31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def normalize_gt(gt, map):\n",
    "\n",
    "    combined_mask = np.zeros(gt.shape[:2], dtype=float)\n",
    "\n",
    "    for gt_id, pred_id in map.items():\n",
    "        if isinstance(gt_id, tuple):\n",
    "            mask = np.zeros(gt.shape[:2], dtype=float)\n",
    "\n",
    "            for ground_truth in gt_id:\n",
    "                mask += np.isin(gt, ground_truth)  \n",
    "        else:\n",
    "            mask = np.isin(gt, gt_id)\n",
    "\n",
    "        if isinstance(pred_id, tuple):\n",
    "            combined_mask += mask * pred_id[0]\n",
    "        else:\n",
    "            combined_mask += mask * pred_id\n",
    "\n",
    "    cut = int((1920 - 1208) / 2)\n",
    "\n",
    "    test = combined_mask[:, cut:1920-cut]\n",
    "\n",
    "    return test\n",
    "\n",
    "def normalize_result(result, map):\n",
    "    combined_mask = np.zeros(result.shape[:2], dtype=float)\n",
    "    \n",
    "    for _ , cls in map.items():\n",
    "        mask = np.isin(result, cls)\n",
    "        \n",
    "        if isinstance(cls, tuple):\n",
    "            combined_mask += mask * cls[0]\n",
    "        else:\n",
    "            combined_mask += mask * cls\n",
    "    \n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee7773-f13f-4712-a355-072129230001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file = '/project_ghent/woverbie/code/configs/config.py'\n",
    "# checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "# model_og = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "# for i, block in enumerate(model_og.backbone.layers):\n",
    "#     # Assuming the transformer blocks are in backbone.layers\n",
    "#     model_og.backbone.layers[i] = CustomTransformerBlock(block, i)\n",
    "#     model_og.backbone.layers[i].r = 1\n",
    "#     # print(model_og.backbone.layers[i].r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a435c16-3003-4745-ac4c-c89edf2482cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Test with an inference function\n",
    "import cv2\n",
    "\n",
    "imgs = sorted(glob('../dataset/camera_lidar/20180810_150607/camera/cam_front_center/*.png'))\n",
    "\n",
    "labels = sorted(glob(\"../dataset/labels/20181204_135952/*\"))\n",
    "\n",
    "perc_list = []\n",
    "\n",
    "prunes = []\n",
    "\n",
    "reduced_tokens_heatmap = None\n",
    "\n",
    "# Display the image\n",
    "for i, (img_path, label_path) in enumerate(zip(imgs, labels)):\n",
    "    \n",
    "\n",
    "    print('image', i)\n",
    "    \n",
    "    result = inference_segmentor(model, img_path)\n",
    "    result_og = inference_segmentor(model_og, img_path)\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # perc = similarity_percentage(gt_norm, res_norm)\n",
    "\n",
    "    # perc_list.append(perc)\n",
    "\n",
    "    # overlap(image_rgb, result[0])\n",
    "    # overlap(image_rgb, result_og[0])\n",
    "\n",
    "    result1 = result[0] - result_og[0]\n",
    "    binary_result = (result1 != 0).astype(int)\n",
    "    \n",
    "    # overlap(image_rgb, binary_result)\n",
    "    percentage_ones = (binary_result.sum() / binary_result.size) * 100\n",
    "    perc_list.append(percentage_ones)\n",
    "\n",
    "    reduction = []\n",
    "\n",
    "    for i, layer in enumerate(model.backbone.layers):\n",
    "        # print(i)\n",
    "        \n",
    "        \n",
    "        if layer.unm_idx is not None:\n",
    "            \n",
    "            # print('unm', layer.unm_idx.shape[1])\n",
    "    \n",
    "            if layer.src_idx is not None:\n",
    "                # print('src', layer.src_idx.shape[1])\n",
    "                reduction.append(layer.src_idx.shape[1])\n",
    "\n",
    "            else:\n",
    "                reduction.append(0)\n",
    "\n",
    "    prunes.append(reduction)\n",
    "\n",
    "    pruned_tokens = []\n",
    "    for i, layer in reversed(list(enumerate(model.backbone.layers))):\n",
    "        if layer.unm_idx is not None:\n",
    "            if len(pruned_tokens) == 0:\n",
    "                pruned_tokens = [0] * len(layer.unm_idx[0])\n",
    "\n",
    "            if layer.src_idx is not None:\n",
    "                # print(layer.src_idx)\n",
    "                temp = pruned_tokens\n",
    "\n",
    "                pruned_tokens = [0] * ( len(layer.unm_idx[0]) + len(layer.src_idx[0]))\n",
    "\n",
    "\n",
    "                for i, unm_idx in enumerate(layer.unm_idx[0]):\n",
    "                    # print('a', unm_idx)\n",
    "                    pruned_tokens[unm_idx] = temp[i]\n",
    "\n",
    "                for i, src_idx in enumerate(layer.src_idx[0]):\n",
    "                    pruned_tokens[src_idx] = 1\n",
    "\n",
    "    #     print(len(pruned_tokens))\n",
    "\n",
    "    # print(pruned_tokens)\n",
    "\n",
    "    if len(pruned_tokens) == 0:\n",
    "        pruned_tokens = [0]*1024\n",
    "        \n",
    "    tokens_reduced = np.array(pruned_tokens).reshape((32, 32))\n",
    "    \n",
    "    # scaled_tokens_reduced = np.repeat(np.repeat(tokens_reduced, 16, axis=0), 16, axis=1)\n",
    "    zoom_factor = 1208 / 32\n",
    "    scaled_tokens_reduced = zoom(tokens_reduced, zoom=(zoom_factor, zoom_factor), order=1)\n",
    "    # print('tokens_reduced', pruned_tokens.count(1))\n",
    "    # plt.imshow(tokens_reduced)\n",
    "    # plt.show()\n",
    "\n",
    "    if reduced_tokens_heatmap is not None:\n",
    "        reduced_tokens_heatmap += scaled_tokens_reduced\n",
    "    else:\n",
    "        reduced_tokens_heatmap = scaled_tokens_reduced\n",
    "\n",
    "    # overlap(image_rgb, scaled_tokens_reduced)\n",
    "\n",
    "\n",
    "\n",
    "encode_times_np = np.array(model.encode_times)\n",
    "np.save(\"encode_times_cpu.npy\", encode_times_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48826c-9d62-4aa8-9766-a0485fa07d2d",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2a9fd-0e32-4d5c-a13d-98650abf6a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.backbone.layers[0].token_history_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934d95d-b485-4075-b453-0491e5a89352",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(perc_list, bins=num_bins, edgecolor='black', alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"loss between no reduction  and reduction (in percentage)\", fontsize=16)\n",
    "plt.xlabel(\"time\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xlim((0,50))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "perc_np = np.array(perc_list)\n",
    "\n",
    "np.save(\"perc_loss_cpu.npy\", perc_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dee9bf-62ec-4f09-820e-07d1ec4c72c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ee168-042a-4437-b6a0-38f5e84631d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 1000\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.hist(model.backbone.reconstruction_times, bins=num_bins, edgecolor='red', alpha=0.75)\n",
    "plt.hist(model.encode_times, bins=num_bins, edgecolor='black', alpha=0.75)\n",
    "# plt.hist(model_og.encode_times, bins=num_bins,color='red', edgecolor='red', alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Models encode times Distribution of Values\", fontsize=16)\n",
    "plt.xlabel(\"time\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.xlim((0.00,0.12))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036484af-fee7-4a5e-b0ed-43baee88857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(prunes, axis=1)\n",
    "\n",
    "# Plot the sums as a line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sums, label='Sum over 2nd dimension')\n",
    "# plt.xlabel('Index of the 1st Dimension')\n",
    "# plt.ylabel('Sum of the 2nd Dimension')\n",
    "plt.title('Change in reduction Over the 2nd Dimension')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "tokens_pruned_np = np.array(sums)\n",
    "\n",
    "np.save(\"tokens_pruned_cpu.npy\", tokens_pruned_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c719541-ce4d-4ac2-be96-90c35805f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "moving_avg = [\n",
    "    np.mean(sums[max(0, i - window_size + 1):i + 1]) \n",
    "    for i in range(len(sums))\n",
    "]\n",
    "\n",
    "# Plot the moving average as a line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(moving_avg, label='Moving Average (window size = 30)')\n",
    "plt.title('Change in Reduction Over the 2nd Dimension')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3ff0b-f8e4-4758-8b4e-afcfd1c5ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_averages = np.mean(prunes, axis=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(column_averages, marker='o', label='Column Averages')\n",
    "plt.title('reductions per layer')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "reduction_per_layer_np = np.array(column_averages)\n",
    "\n",
    "np.save(\"reductions_per_layer_cpu.npy\", reduction_per_layer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0753ada-addc-49d2-a427-f9aeabd9d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 1000\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.hist(s, bins=num_bins, edgecolor='black', alpha=0.75)\n",
    "plt.hist(model.decode_times, bins=num_bins, edgecolor='red', alpha=0.3)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Models decode times Distribution of Values\", fontsize=16)\n",
    "plt.xlabel(\"time\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.xlim((0.02,0.06))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b68d4-5f7a-4c06-b20f-27d2f5b94bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of tokens reduced\n",
    "\n",
    "# sum_ones = np.sum(reduced_tokens_heatmap, axis=0)\n",
    "\n",
    "# Step 2: Normalize by the total number of frames\n",
    "# num_frames = reduced_tokens_heatmap.shape[0]\n",
    "# heatmap = sum_ones / num_frames  # This gives a 2D array with values between 0 and 1\n",
    "\n",
    "# Step 3: Visualize as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(reduced_tokens_heatmap, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Frequency of reduction')\n",
    "plt.title('Frequency Heatmap of reduced tokens')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n",
    "\n",
    "reduced_tokens_heatmap_np = np.array(reduced_tokens_heatmap)\n",
    "\n",
    "np.save(\"reduced_tokens_heatmap_np_cpu.npy\", reduced_tokens_heatmap_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e232671-0403-4565-b98d-ef482d85cd57",
   "metadata": {},
   "source": [
    "# flops calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a0e781-77e7-4dc5-93dc-ba4431506eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/project_ghent/woverbie/code/configs/config.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "# Setting the r values\n",
    "\n",
    "r_values = np.linspace(0.995, 0.93, len(model.backbone.layers)).tolist()\n",
    "for i, block in enumerate(model.backbone.layers):\n",
    "    # Assuming the transformer blocks are in backbone.layers\n",
    "    model.backbone.layers[i].r = r_values[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb6272-cdb6-4d00-b174-8b8aa7be3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Test with an inference function\n",
    "import cv2\n",
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from mmseg.datasets.pipelines import Compose\n",
    "\n",
    "imgs = sorted(glob('../dataset/camera_lidar/20180810_150607/camera/cam_front_center/*.png'))\n",
    "\n",
    "cfg = model.cfg  # The config file used to initialize the model\n",
    "test_pipeline = Compose(cfg.data.test.pipeline)\n",
    "\n",
    "perc_list = []\n",
    "\n",
    "prunes = []\n",
    "\n",
    "reduced_tokens_heatmap = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def profile_inference(model, img):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], \n",
    "                 with_flops=True,  # Enable FLOP counting\n",
    "                 record_shapes=True) as prof:\n",
    "        # Perform inference\n",
    "        _ = inference_segmentor(model, img)\n",
    "\n",
    "    # Process and summarize the profiling data\n",
    "    # flops = prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10)\n",
    "    # print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))\n",
    "    return prof\n",
    "\n",
    "\n",
    "prof_list = []\n",
    "\n",
    "# Display the image\n",
    "for i, (img_path, label_path) in enumerate(zip(imgs, labels)):\n",
    "\n",
    "    prof = profile_inference(model, img_path)\n",
    "    \n",
    "    avg = sum(op.flops for op in prof.key_averages() if hasattr(op, 'flops'))\n",
    "\n",
    "    print(avg)\n",
    "\n",
    "    prof_list.append(avg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb18179-f1db-4674-b834-6a92dae92c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sums as a line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prof_list, label='flops')\n",
    "# plt.xlabel('Index of the 1st Dimension')\n",
    "# plt.ylabel('Sum of the 2nd Dimension')\n",
    "plt.title('flops')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# perc_np = np.array(prof_list)\n",
    "\n",
    "np.save(\"prof_list.npy\", prof_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f46d5-2a5e-4ebd-b4b9-51c057464e42",
   "metadata": {},
   "source": [
    "# imports form numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f5d22-09cf-4fd2-9d24-a66f6cf6187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_list = np.load('./prof_list.npy')\n",
    "tokens_pruned = np.load('./tokens_pruned.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea988-840c-41e6-9cb9-b63aa961dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66724d9-a56b-4cc8-ad70-e09a5c1663aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "# tokens_pruned = np.array([0, 256, 512, 768, 1024])  # Max = 1024\n",
    "# prof_list = np.array([0, 195955603867, 391911207734, 587866811601, 783822415468])  # Max = 783822415468\n",
    "\n",
    "window_size = 2\n",
    "moving_avg_tokens = [\n",
    "    np.mean(tokens_pruned[max(0, i - window_size + 1):i + 1]) \n",
    "    for i in range(len(tokens_pruned))\n",
    "]\n",
    "\n",
    "moving_avg_prof = [\n",
    "    np.mean(prof_list[max(0, i - window_size + 1):i + 1]) \n",
    "    for i in range(len(prof_list))\n",
    "]\n",
    "\n",
    "# Create the figure and first axis\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot tokens_pruned on the first axis\n",
    "ax1.plot(moving_avg_tokens, label=\"Tokens Pruned\", color=\"blue\")\n",
    "ax1.set_xlabel(\"Index\")\n",
    "ax1.set_ylabel(\"Tokens Pruned\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "ax1.set_ylim(0, 1024)\n",
    "\n",
    "# Create the second axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot prof_list on the second axis\n",
    "ax2.plot(moving_avg_prof, label=\"Prof List\", color=\"orange\")\n",
    "ax2.set_ylabel(\"Prof List\", color=\"orange\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"orange\")\n",
    "ax2.set_ylim(0, 783822415468)\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Overlay of Two Data Sources with Twin Axes\")\n",
    "\n",
    "# Add a grid for clarity\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7331b-9b7b-45a0-9087-23d71dfb3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Model is on device: {next(model_og.parameters()).device}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. The model is likely on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb176ca2-7019-4e54-8f8c-0198e4e563cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33ba2e-551d-4e59-80d6-f9f7b1e38892",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "\n",
    "encode_times_1 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_1.0.npy')\n",
    "encode_times_98 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.98.npy')\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(encode_times_1, bins=num_bins, edgecolor='red', alpha=0.75)\n",
    "plt.hist(encode_times_98, bins=num_bins, edgecolor='black', alpha=0.75)\n",
    "# plt.hist(model_og.encode_times, bins=num_bins,color='red', edgecolor='red', alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Models encode times Distribution of Values\", fontsize=16)\n",
    "plt.xlabel(\"time\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xlim((0.00,0.15))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cc3d4-6f89-4f8e-88e4-3ddadf7eae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4724ebe-a928-4acc-a6e9-7bfbd023174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_GAIMERSHEIM = sorted(glob('../dataset/camera_lidar/20180810_150607/camera/cam_front_center/*.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f17ed4-a82c-488d-b8f5-3f34bcff975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(VIDEO_GAIMERSHEIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460b042-83ba-41e6-95eb-545082fe65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "num_bins = 1000\n",
    "\n",
    "encode_times_1 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_1.0.npy')\n",
    "encode_times_98 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.98.npy')\n",
    "encode_times_96 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.96.npy')\n",
    "encode_times_94 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.94.npy')\n",
    "encode_times_92 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.92.npy')\n",
    "encode_times_90 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.9.npy')\n",
    "encode_times_88 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.88.npy')\n",
    "encode_times_86 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.86.npy')\n",
    "encode_times_84 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_0.84.npy')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# KDE for smooth line\n",
    "kde_1 = gaussian_kde(encode_times_1)\n",
    "kde_98 = gaussian_kde(encode_times_98)\n",
    "kde_96 = gaussian_kde(encode_times_96)\n",
    "kde_94 = gaussian_kde(encode_times_94)\n",
    "kde_92 = gaussian_kde(encode_times_92)\n",
    "kde_90 = gaussian_kde(encode_times_90)\n",
    "kde_88 = gaussian_kde(encode_times_88)\n",
    "kde_86 = gaussian_kde(encode_times_86)\n",
    "kde_84 = gaussian_kde(encode_times_84)\n",
    "\n",
    "\n",
    "# Generate a range of values for the x-axis\n",
    "x = np.linspace(0.00, 0.1, 500)\n",
    "\n",
    "# Plot the KDEs\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.plot(x, kde_1(x), label=\"Encode Times 1.0\", color='red', alpha=0.75)\n",
    "plt.plot(x, kde_98(x), label=\"threshold = 0.98\", color='black', alpha=0.75)\n",
    "plt.plot(x, kde_96(x), label=\"threshold = 0.96\", color='green', alpha=0.75)\n",
    "plt.plot(x, kde_94(x), label=\"threshold = 0.94\", color='blue', alpha=0.75)\n",
    "plt.plot(x, kde_92(x), label=\"threshold = 0.92\", color='orange', alpha=0.75)\n",
    "plt.plot(x, kde_90(x), label=\"threshold = 0.90\", color='yellow', alpha=0.75)\n",
    "plt.plot(x, kde_88(x), label=\"threshold = 0.88\", color='pink', alpha=0.75)\n",
    "plt.plot(x, kde_86(x), label=\"threshold = 0.86\", color='gray', alpha=0.75)\n",
    "plt.plot(x, kde_84(x), label=\"threshold = 0.84\", color='purple', alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Models Encode Times Distribution of Values\", fontsize=16)\n",
    "plt.xlabel(\"Time\", fontsize=14)\n",
    "plt.ylabel(\"Density\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.xlim((0.00, 0.15))\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cc120-abdc-4167-8051-642874b1ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7decd01-4ea0-446c-82f9-356169590aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "thresholds = [0.98, 0.96, 0.94, 0.92, 0.90, 0.88, 0.86, 0.84]\n",
    "encode_times = {\n",
    "    threshold: np.load(f'/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_{threshold}.npy')\n",
    "    for threshold in thresholds\n",
    "}\n",
    "\n",
    "# Calculate mean and median for each threshold\n",
    "means = [np.mean(encode_times[threshold]) for threshold in thresholds]\n",
    "medians = [np.median(encode_times[threshold]) for threshold in thresholds]\n",
    "\n",
    "# Plot mean and median\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, means, label=\"Mean Encode Time\", marker='o', linestyle='-', color='blue')\n",
    "plt.plot(thresholds, medians, label=\"Median Encode Time\", marker='s', linestyle='--', color='orange')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Mean and Median Encode Times for Different Thresholds\", fontsize=16)\n",
    "plt.xlabel(\"Threshold\", fontsize=14)\n",
    "plt.ylabel(\"Encode Time (s)\", fontsize=14)\n",
    "plt.ylim(0,0.06)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xticks(thresholds)  # Ensure all thresholds are labeled on the x-axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b05444-dc25-412b-a898-17060915ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encode_times_84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbccf1e-106c-467d-9219-fd046c5b00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encode_times_96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74c670-6453-4671-8c9c-d600a26142b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(encode_times_84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11270ffb-23ca-4153-aa60-7817036c69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_token_heatmap = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/reduced_tokens_heatmap_lin_0.995_0.93.npy')\n",
    "\n",
    "# reduced_token_heatmap = reduced_token_heatmap / len(encode_times_98)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(reduced_token_heatmap, cmap='hot', interpolation='nearest', vmin=0, vmax=len(encode_times_98))\n",
    "# plt.colorbar(label='#Frames where token is reduced')\n",
    "plt.title('Frequency Heatmap of Reduced Tokens \\n with a Linear Threshold')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n",
    "\n",
    "reduced_token_heatmap = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/reduced_tokens_heatmap_all_layers_0.98.npy')\n",
    "\n",
    "# reduced_token_heatmap = reduced_token_heatmap / len(encode_times_98)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(reduced_token_heatmap, cmap='hot', interpolation='nearest', vmin=0, vmax=len(encode_times_98))\n",
    "# plt.colorbar(label='#Frames where token is reduced')\n",
    "plt.title('Frequency Heatmap of Reduced Tokens \\n with a Fixed Threshold = 0.98')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n",
    "\n",
    "reduced_token_heatmap = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/reduced_tokens_heatmap_all_layers_0.92.npy')\n",
    "\n",
    "# reduced_token_heatmap = reduced_token_heatmap / len(encode_times_98)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(reduced_token_heatmap, cmap='hot', interpolation='nearest', vmin=0, vmax=len(encode_times_98))\n",
    "plt.colorbar(label='#Frames where token is reduced')\n",
    "plt.title('Frequency Heatmap of Reduced \\n Tokens with a Fixed Threshold = 0.9')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f7776-4c49-401b-a3b6-39862e6ad037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_98 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.98.npy')\n",
    "# acc_96 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.96.npy')\n",
    "# acc_94 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.94.npy')\n",
    "# acc_92 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.92.npy')\n",
    "# acc_90 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.9.npy')\n",
    "# acc_88 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.88.npy')\n",
    "# acc_86 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.86.npy')\n",
    "# acc_84 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_0.84.npy')\n",
    "\n",
    "# # KDE for smooth line\n",
    "# # kde_1 = gaussian_kde(acc_1)\n",
    "# kde_98 = gaussian_kde(acc_98)\n",
    "# kde_96 = gaussian_kde(acc_96)\n",
    "# kde_94 = gaussian_kde(acc_94)\n",
    "# kde_92 = gaussian_kde(acc_92)\n",
    "# kde_90 = gaussian_kde(acc_90)\n",
    "# kde_88 = gaussian_kde(acc_88)\n",
    "# kde_86 = gaussian_kde(acc_86)\n",
    "# kde_84 = gaussian_kde(acc_84)\n",
    "\n",
    "# # Generate a range of values for the x-axis\n",
    "# x = np.linspace(0, 100, 500)\n",
    "\n",
    "# # Plot the KDEs\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plt.plot(x, kde_98(x), label=\"Loss r=0.98\", color='black', alpha=0.75)\n",
    "# plt.plot(x, kde_96(x), label=\"Loss r=0.96\", color='green', alpha=0.75)\n",
    "# plt.plot(x, kde_94(x), label=\"Loss r=0.94\", color='blue', alpha=0.75)\n",
    "# plt.plot(x, kde_92(x), label=\"Loss r=0.92\", color='orange', alpha=0.75)\n",
    "# plt.plot(x, kde_90(x), label=\"Loss r=0.90\", color='red', alpha=0.75)\n",
    "# plt.plot(x, kde_88(x), label=\"Loss r=0.88\", color='pink', alpha=0.75)\n",
    "# plt.plot(x, kde_86(x), label=\"Loss r=0.86\", color='gray', alpha=0.75)\n",
    "# plt.plot(x, kde_84(x), label=\"Loss r=0.84\", color='purple', alpha=0.75)\n",
    "\n",
    "\n",
    "\n",
    "# # Formatting the plot\n",
    "# plt.title(\"Pixel-wise Loss compared to original model\", fontsize=16)\n",
    "# plt.xlabel(\"Loss (in %)\", fontsize=14)\n",
    "# plt.ylabel(\"Percentage of processed frames\", fontsize=14)\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f20f28-a9e0-4e08-9772-c2d51a093cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb8d3a-526b-4e43-a088-5f6a26645c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed430d-f5a6-4e9c-bc9c-d516d983d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(acc_84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c33343-c68e-41a1-9fdd-964386b7d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"98\", round(np.mean(acc_98),2), round(np.median(acc_98),2))\n",
    "print(\"96\", round(np.mean(acc_96),2), round(np.median(acc_96),2))\n",
    "print(\"94\", round(np.mean(acc_94),2), round(np.median(acc_94),2))\n",
    "print(\"92\", round(np.mean(acc_92),2), round(np.median(acc_92),2))\n",
    "print(\"90\", round(np.mean(acc_90),2), round(np.median(acc_90),2))\n",
    "print(\"88\", round(np.mean(acc_88),2), round(np.median(acc_88),2))\n",
    "print(\"86\", round(np.mean(acc_86),2), round(np.median(acc_86),2))\n",
    "print(\"84\", round(np.mean(acc_84),2), round(np.median(acc_84),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb22485-b05d-4092-90a2-a4215ac3b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_bins = 1000\n",
    "\n",
    "# tokens_pruned_98 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/tokens_pruned_0.98.npy')\n",
    "# tokens_pruned_98 = np.sum(tokens_pruned_98, axis=1)\n",
    "\n",
    "# # Plot the sums as a line chart\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(tokens_pruned_98, label='# Reduced tokens for frame')\n",
    "# plt.title('threshold = 0.98',  fontsize=25)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# tokens_pruned_92 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/tokens_pruned_0.92.npy')\n",
    "# tokens_pruned_92 = np.sum(tokens_pruned_92, axis=1)\n",
    "\n",
    "# # Plot the sums as a line chart\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(tokens_pruned_92, label='Sum over 2nd dimension')\n",
    "# plt.title('threshold = 0.92',  fontsize=25)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# tokens_pruned_84 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/tokens_pruned_0.84.npy')\n",
    "# tokens_pruned_84 = np.sum(tokens_pruned_84, axis=1)\n",
    "\n",
    "# # Plot the sums as a line chart\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(tokens_pruned_84, label='Sum over 2nd dimension')\n",
    "# # plt.xlabel('Index of the 1st Dimension')\n",
    "# # plt.ylabel('Sum of the 2nd Dimension')\n",
    "# plt.title('threshold = 0.84',  fontsize=25)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plt.plot(tokens_pruned_84, label='Threshold = 0.84', alpha=0.75)\n",
    "# plt.plot(tokens_pruned_92, label='Threshold = 0.92',alpha=0.75)\n",
    "# plt.plot(tokens_pruned_98, label='Threshold = 0.98', alpha=0.75)\n",
    "# plt.plot([1024] * len(tokens_pruned_84), label='Max tokens to reduce', alpha=1)\n",
    "# plt.title('Reduced Tokens through Video Frames')\n",
    "# plt.xlabel(\"Frame index\", fontsize=14)\n",
    "# plt.ylabel(\"# Reduced Tokens\", fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74646671-904a-44f9-a3ed-aafb134049f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_pruned_98)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d46797-b602-480b-aeee-f1651a51e673",
   "metadata": {},
   "source": [
    "# trying to get where it sometimes is very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b24cc4-570d-4885-9e14-a3b34e1f0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pruned_96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bac160-1cec-40ee-9535-b1dfa6e28b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokens_pruned_98[13000:15688]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6c2d3-6bbe-4caa-8a1a-35614963e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pruned_98 = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/tokens_pruned_0.98.npy')\n",
    "tokens_pruned_98 = np.sum(tokens_pruned_98, axis=1)\n",
    "\n",
    "# Plot the sums as a line chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tokens_pruned_98, label='# Reduced tokens for frame')\n",
    "plt.title('Change in Reduction of Tokens through Frames')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Frames\", fontsize=14)\n",
    "plt.ylabel(\"# reduced tokens\", fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(tokens_pruned_98, label='# Reduced tokens for frame')\n",
    "plt.title('Change in Reduction of Tokens through Frames')\n",
    "plt.xlim(15000,15500)\n",
    "plt.ylim(0,1030)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Frames\", fontsize=14)\n",
    "plt.ylabel(\"# reduced tokens\", fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692fb5b-dd5b-47f7-9664-d1bd410389e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_path in enumerate(VIDEO_GAIMERSHEIM):\n",
    "    if i % 30 == 0 and i>3000:\n",
    "        img = mpimg.imread(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"frame {i}\")\n",
    "        plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e95ff5-b36d-49eb-84a0-3a82a01534e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pruned = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/tokens_pruned_lin_0.995_0.93.npy')\n",
    "\n",
    "\n",
    "column_averages = np.mean(tokens_pruned, axis=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(column_averages, marker='o', label='Column Averages')\n",
    "plt.title('reductions per layer')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "reduction_per_layer_np = np.array(column_averages)\n",
    "\n",
    "np.save(\"reductions_per_layer_cpu.npy\", reduction_per_layer_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018ff74-ab4f-4568-b558-4d787de798a3",
   "metadata": {},
   "source": [
    "# The OG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebc9c3-7e26-4793-a2ba-8066b061228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/project_ghent/woverbie/code/SegVit/configs/segvit/segvit_vit-l_jax_512x512_80k_cocostuff10k.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "model_og = init_segmentor(config_file, checkpoint_file, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c1b0c4-c5ee-4031-88b1-e8193ff41354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = sorted(glob('../dataset/camera_lidar/20180810_150607/camera/cam_front_center/*.png'))\n",
    "imgs = imgs[0:3]\n",
    "\n",
    "for img in imgs:\n",
    "    result = inference_segmentor(model_og, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df999b6-9539-406d-999e-5eeb5658ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_og.encode_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf73cd-4d6d-4e19-8b61-f6a974e33923",
   "metadata": {},
   "source": [
    "# different reduction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5267283-33be-4922-9467-6366b10a7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "thresholds = [1,2,3,4,6,8,12,24]\n",
    "encode_times = {\n",
    "    threshold: np.load(f'/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_lin_int_{threshold}.npy')\n",
    "    for threshold in thresholds\n",
    "}\n",
    "\n",
    "# Prepare the data for histogram line plots\n",
    "labels = [f\"Reduction Interval = {threshold}\" for threshold in thresholds]\n",
    "colors = ['black', 'green', 'blue', 'orange', 'red', 'pink', 'gray', 'purple']\n",
    "datasets = [encode_times[threshold] for threshold in thresholds]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Generate and plot histogram data as lines\n",
    "bins = np.linspace(0, 0.1, 50)  # Define bins (same for all datasets)\n",
    "for data, label, color in zip(datasets, labels, colors):\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Compute bin centers\n",
    "    plt.plot(bin_centers, counts, label=label, color=color, alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Encode Times Distribution\", fontsize=16)\n",
    "plt.xlabel(\"Encode Time (s)\", fontsize=14)\n",
    "plt.ylabel(\"# Frames\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd68ae7-126b-43ec-8eaa-8be54da33d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "thresholds = [1,2,3,4,6,8,12,24]\n",
    "encode_times = {\n",
    "    threshold: np.load(f'/project_ghent/woverbie/code/notebooks/benchmarking/encode_times_lin_int_{threshold}.npy')\n",
    "    for threshold in thresholds\n",
    "}\n",
    "\n",
    "# Calculate mean and median for each threshold\n",
    "means = [np.mean(encode_times[threshold]) for threshold in thresholds]\n",
    "medians = [np.median(encode_times[threshold]) for threshold in thresholds]\n",
    "\n",
    "# Plot mean and median\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(thresholds, means, label=\"Mean Encode Time\", marker='o', linestyle='-', color='blue')\n",
    "plt.plot(thresholds, medians, label=\"Median Encode Time\", marker='s', linestyle='--', color='orange')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Mean and Median Encode Times \\n for Different Thresholds\", fontsize=16)\n",
    "plt.xlabel(\"Threshold\", fontsize=14)\n",
    "plt.ylabel(\"Encode Time (s)\", fontsize=14)\n",
    "plt.ylim(0,0.06)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xticks(thresholds)  # Ensure all thresholds are labeled on the x-axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85dd36-c529-47da-a1b0-593352006f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(arr):\n",
    "    mean = np.mean(arr)\n",
    "    median = np.median(arr)\n",
    "    std_dev = np.std(arr)\n",
    "    variance = np.var(arr)\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    range_val = np.ptp(arr)  # Range (max - min)\n",
    "    sum_val = np.sum(arr)\n",
    "    count = len(arr)  # Number of elements\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Mean: {mean}\")\n",
    "    print(f\"Median: {median}\")\n",
    "    print(f\"Standard Deviation: {std_dev}\")\n",
    "    print(f\"Variance: {variance}\")\n",
    "    print(f\"Minimum: {min_val}\")\n",
    "    print(f\"Maximum: {max_val}\")\n",
    "    print(f\"Range: {range_val}\")\n",
    "    print(f\"Sum: {sum_val}\")\n",
    "    print(f\"Count: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a9812-879e-4f9f-8b3f-e8ad3a5887b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(encode_times_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af650c7-3668-4ed4-adcd-9a96dba66416",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(encode_times_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69305141-51e0-4198-9c25-2c445a10f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(encode_times_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18672fc-caa5-4ad9-9be6-09a2f6fdabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pruned = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/tokens_pruned_lin_int_1.npy')\n",
    "\n",
    "\n",
    "column_averages = np.mean(tokens_pruned, axis=0)\n",
    "column_median = np.median(tokens_pruned, axis=0)\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "plt.plot(column_median, label=\"Median Reduction of Tokens in Layer\", marker='s', linestyle='--', color='orange')\n",
    "plt.plot(column_averages, marker='o', label='Mean Reduction of Tokens in Layer')\n",
    "plt.title('Token Reduction per Layer')\n",
    "plt.xlabel('Layers in Vision Transformer Encoder')\n",
    "plt.ylabel('# Reduced Tokens in Layer')\n",
    "plt.legend()\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# reduction_per_layer_np = np.array(column_averages)\n",
    "\n",
    "# np.save(\"reductions_per_layer_cpu.npy\", reduction_per_layer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d178a3-7608-4dc5-acb8-783324fed4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "thresholds = [1,2,3,4,6,8,12,24]\n",
    "encode_times = {\n",
    "    threshold: np.load(f'/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_lin_int_{threshold}.npy')\n",
    "    for threshold in thresholds\n",
    "}\n",
    "\n",
    "# Calculate mean and median for each threshold\n",
    "means = [np.mean(encode_times[threshold]) for threshold in thresholds]\n",
    "medians = [np.median(encode_times[threshold]) for threshold in thresholds]\n",
    "\n",
    "# Plot mean and median\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(thresholds, means, label=\"Mean Encode Time\", marker='o', linestyle='-', color='blue')\n",
    "plt.plot(thresholds, medians, label=\"Median Encode Time\", marker='s', linestyle='--', color='orange')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Mean and Median Pixel-Wise Loss \\n for Different Reduction Intervals\", fontsize=16)\n",
    "plt.xlabel(\"Reduction Interval\", fontsize=14)\n",
    "plt.ylabel(\"Pixel-Wise Loss (%)\", fontsize=14)\n",
    "plt.ylim(0,10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.xticks(thresholds)  # Ensure all thresholds are labeled on the x-axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d735e82-08cf-4b2c-9655-94ff9cd6b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "thresholds = [1,2,3,4,6,8,12,24]\n",
    "encode_times = {\n",
    "    threshold: np.load(f'/project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_lin_int_{threshold}.npy')\n",
    "    for threshold in thresholds\n",
    "}\n",
    "\n",
    "# Prepare the data for histogram line plots\n",
    "labels = [f\"Reduction Interval = {threshold}\" for threshold in thresholds]\n",
    "colors = ['black', 'green', 'blue', 'orange', 'red', 'pink', 'gray', 'purple']\n",
    "datasets = [encode_times[threshold] for threshold in thresholds]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Generate and plot histogram data as lines\n",
    "bins = np.linspace(0, 20, 100)  # Define bins (same for all datasets)\n",
    "for data, label, color in zip(datasets, labels, colors):\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Compute bin centers\n",
    "    plt.plot(bin_centers, counts, label=label, color=color, alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Pixel-Wise Loss Distribution compared to Original Model \\n for Different Reduction Intervals\", fontsize=16)\n",
    "plt.xlabel(\"Pixel-Wise Loss (%)\", fontsize=14)\n",
    "plt.ylabel(\"# Frames\", fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a28b71-33fa-4d00-90a3-904d1e2526cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "num_bins = 1000\n",
    "\n",
    "encode_times = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/model_og_encode_times_gpu.npy')\n",
    "decode_times = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/model_og_decode_times_gpu.npy')\n",
    "\n",
    "# project_ghent/woverbie/code/notebooks/benchmarking/encode_times_lin_int_1.npy\n",
    "# project_ghent/woverbie/code/notebooks/benchmarking/pixel_wise_acc_list_int_0.995_0.93.npy\n",
    "\n",
    "# KDE for smooth line\n",
    "kde_1 = gaussian_kde(encode_times)\n",
    "kde_2 = gaussian_kde(decode_times)\n",
    "\n",
    "\n",
    "# Generate a range of values for the x-axis\n",
    "x = np.linspace(0.00, 0.08, 500)\n",
    "\n",
    "# Plot the KDEs\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.plot(x, kde_1(x), label=\"Encode Times\", color='red', alpha=0.75)\n",
    "plt.plot(x, kde_2(x), label=\"Decode Times\", color='green', alpha=0.75)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title(\"Distribution of the Encode and Decode\\n Times during Inference of the Original Model\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"# Frames\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509179d-37e3-4054-ab3d-b2a92630e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(encode_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524730b-38bd-46fc-86ba-83198b687fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(decode_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa101b6-b7f7-4152-ad52-5347d348ce57",
   "metadata": {},
   "source": [
    "# which tokens are matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437fdad-f85e-4cf0-a0e1-e18a6af211d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5aa16-a6c4-4292-ac24-1db37db53916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
