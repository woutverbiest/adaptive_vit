{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e258a4b1-7473-45b6-a4ff-87f62540087a",
   "metadata": {},
   "source": [
    "\n",
    "<pre>\n",
    "  ____                      _                              _     _               \n",
    " |  _ \\                    | |                            | |   (_)              \n",
    " | |_) |  ___  _ __    ___ | |__   _ __ ___    __ _  _ __ | | __ _  _ __    __ _ \n",
    " |  _ <  / _ \\| '_ \\  / __|| '_ \\ | '_ ` _ \\  / _` || '__|| |/ /| || '_ \\  / _` |\n",
    " | |_) ||  __/| | | || (__ | | | || | | | | || (_| || |   |   < | || | | || (_| |\n",
    " |____/  \\___||_| |_| \\___||_| |_||_| |_| |_| \\__,_||_|   |_|\\_\\|_||_| |_| \\__, |\n",
    "                                                                            __/ |\n",
    "                                                                           |___/ \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee0e96-214b-4660-83ef-62d6f5376cc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b99fbe-a191-4740-b57c-ac739e69f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mmcv\n",
    "# import mmseg\n",
    "from mmseg.apis import inference_segmentor\n",
    "from mmseg.apis import init_segmentor\n",
    "from mmseg.ops import resize\n",
    "\n",
    "import mmcv\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"/project_ghent/woverbie/code/SegVit/decode_heads\")\n",
    "sys.path.append(\"/project_ghent/woverbie/code/SegVit/losses\")\n",
    "\n",
    "import atm_head, tpn_atm_head\n",
    "import atm_loss\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "import math\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef825f9-76c7-402c-adf8-c89769bb311c",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04eb00b-9c18-48bf-83ba-1d57548bee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_MUNICH\n",
    "# VIDEO_INGOLSTADT\n",
    "VIDEO_GAIMERSHEIM = sorted(glob('../dataset/camera_lidar/20180810_150607/camera/cam_front_center/*.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a5d293f-7a45-40e9-8caa-fff651651a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ATM head\n",
      "load checkpoint from local path: ../model/checkpoint/COCOstuff_49.9.pth\n"
     ]
    }
   ],
   "source": [
    "config_file = '../configs/token_reducing_config_gpu.py'\n",
    "checkpoint_file = '../model/checkpoint/COCOstuff_49.9.pth'\n",
    "model_og = init_segmentor(config_file, checkpoint_file, device='cuda:0') # can not be used for time measures!\n",
    "\n",
    "for i, block in enumerate(model_og.backbone.layers):\n",
    "    model_og.backbone.layers[i].r = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70f9bd-e4cb-4c98-8682-f2c19af93582",
   "metadata": {},
   "source": [
    "## Fixed r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cdc44f-c313-4e9e-849a-297130db0f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98, 0.96, 0.94, 0.92, 0.9, 0.88, 0.86, 0.84, 0.82]\n",
      "0.98\n",
      "Initializing ATM head\n",
      "load checkpoint from local path: ../model/checkpoint/COCOstuff_49.9.pth\n",
      "image 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/project_ghent/woverbie/benchmarking/SegVit_test_2/test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(VIDEO_GAIMERSHEIM):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, i)\n\u001b[0;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minference_segmentor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     result_og \u001b[38;5;241m=\u001b[39m inference_segmentor(model_og, img_path)\n\u001b[1;32m     33\u001b[0m     result1 \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m result_og[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mmseg/apis/inference.py:102\u001b[0m, in \u001b[0;36minference_segmentor\u001b[0;34m(model, imgs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mmcv/runner/fp16_utils.py:119\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@auto_fp16 can only be used to decorate the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    117\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod of those classes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16_enabled\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfp16_enabled):\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mold_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[1;32m    122\u001b[0m args_info \u001b[38;5;241m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mmseg/models/segmentors/base.py:110\u001b[0m, in \u001b[0;36mBaseSegmentor.forward\u001b[0;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_train(img, img_metas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/mmseg/models/segmentors/base.py:92\u001b[0m, in \u001b[0;36mBaseSegmentor.forward_test\u001b[0;34m(self, imgs, img_metas, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(shape \u001b[38;5;241m==\u001b[39m pad_shapes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m pad_shapes)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_augs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_metas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug_test(imgs, img_metas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/tmp/tmp0grx9l_c/tmpxlr6gu4w.py:322\u001b[0m, in \u001b[0;36msimple_test\u001b[0;34m(self, img, img_meta, rescale)\u001b[0m\n",
      "File \u001b[0;32m/tmp/tmp0grx9l_c/tmpxlr6gu4w.py:304\u001b[0m, in \u001b[0;36minference\u001b[0;34m(self, img, img_meta, rescale)\u001b[0m\n",
      "File \u001b[0;32m/tmp/tmp0grx9l_c/tmpxlr6gu4w.py:253\u001b[0m, in \u001b[0;36mwhole_inference\u001b[0;34m(self, img, img_meta, rescale)\u001b[0m\n",
      "File \u001b[0;32m/tmp/tmp0grx9l_c/tmpxlr6gu4w.py:81\u001b[0m, in \u001b[0;36mencode_decode\u001b[0;34m(self, img, img_metas)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/project_ghent/woverbie/benchmarking/SegVit_test_2/test.txt'"
     ]
    }
   ],
   "source": [
    "config_file = '../configs/token_reducing_config_gpu.py'\n",
    "checkpoint_file = '../model/checkpoint/COCOstuff_49.9.pth'\n",
    "\n",
    "r_values = np.arange(0.98, 0.82, -0.02).round(2).tolist()\n",
    "\n",
    "print(r_values)\n",
    "\n",
    "for r_value in r_values:\n",
    "    print(r_value)\n",
    "\n",
    "    # model\n",
    "    model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "    # Overwriting the r values\n",
    "    for i, block in enumerate(model.backbone.layers):\n",
    "        model.backbone.layers[i].r = r_value\n",
    "        model.backbone.layers[i].reduction_interval = 1\n",
    "\n",
    "    pixel_wise_acc_list = []\n",
    "    \n",
    "    prunes = []\n",
    "\n",
    "    reduced_tokens_heatmap = None\n",
    "    \n",
    "    # Display the image\n",
    "    for i, img_path in enumerate(VIDEO_GAIMERSHEIM):\n",
    "    \n",
    "        print('image', i)\n",
    "        \n",
    "        result = inference_segmentor(model, img_path)\n",
    "        result_og = inference_segmentor(model_og, img_path)\n",
    "    \n",
    "        result1 = result[0] - result_og[0]\n",
    "        binary_result = (result1 != 0).astype(int)\n",
    "        \n",
    "        percentage_ones = (binary_result.sum() / binary_result.size) * 100\n",
    "        pixel_wise_acc_list.append(percentage_ones)\n",
    "    \n",
    "        reduction = []\n",
    "    \n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "    \n",
    "            if layer.unm_idx is not None:\n",
    "                \n",
    "                if layer.src_idx is not None:\n",
    "                    reduction.append(layer.src_idx.shape[1])\n",
    "                else:\n",
    "                    reduction.append(0)\n",
    "    \n",
    "        prunes.append(reduction)\n",
    "\n",
    "        pruned_tokens = []\n",
    "        for i, layer in reversed(list(enumerate(model.backbone.layers))):\n",
    "            if layer.unm_idx is not None:\n",
    "                if len(pruned_tokens) == 0:\n",
    "                    pruned_tokens = [0] * len(layer.unm_idx[0])\n",
    "    \n",
    "                if layer.src_idx is not None:\n",
    "                    temp = pruned_tokens\n",
    "    \n",
    "                    pruned_tokens = [0] * ( len(layer.unm_idx[0]) + len(layer.src_idx[0]))\n",
    "    \n",
    "                    for i, unm_idx in enumerate(layer.unm_idx[0]):\n",
    "                        pruned_tokens[unm_idx] = temp[i]\n",
    "    \n",
    "                    for i, src_idx in enumerate(layer.src_idx[0]):\n",
    "                        pruned_tokens[src_idx] = 1\n",
    "    \n",
    "        if len(pruned_tokens) == 0:\n",
    "            pruned_tokens = [0]*1024\n",
    "            \n",
    "        tokens_reduced = np.array(pruned_tokens).reshape((32, 32))\n",
    "        \n",
    "        # scaled_tokens_reduced = np.repeat(np.repeat(tokens_reduced, 16, axis=0), 16, axis=1)\n",
    "        zoom_factor = 1208 / 32\n",
    "        scaled_tokens_reduced = zoom(tokens_reduced, zoom=(zoom_factor, zoom_factor), order=1)\n",
    "\n",
    "        if reduced_tokens_heatmap is not None:\n",
    "            reduced_tokens_heatmap += scaled_tokens_reduced\n",
    "        else:\n",
    "            reduced_tokens_heatmap = scaled_tokens_reduced\n",
    "\n",
    "    # prunes\n",
    "    np.save(f\"benchmarking/tokens_pruned_all_layers_{r_value}.npy\", prunes)\n",
    "    \n",
    "    # pixel_wise_acc_list\n",
    "    np.save(f\"benchmarking/pixel_wise_acc_list_all_layers_{r_value}.npy\", pixel_wise_acc_list)\n",
    "\n",
    "    # encode times\n",
    "    encode_times_np = np.array(model.encode_times)\n",
    "    np.save(f\"benchmarking/encode_times_all_layers_{r_value}.npy\", encode_times_np)\n",
    "\n",
    "    # heatmap\n",
    "    np.save(f\"benchmarking/reduced_tokens_heatmap_all_layers_{r_value}.npy\", reduced_tokens_heatmap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906a242-3faa-4819-95f9-f73a7d96e579",
   "metadata": {},
   "source": [
    "## Linear r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9300a3-832d-47f3-80f3-16303d954586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "r_values = np.linspace(0.995, 0.93, len(model.backbone.layers)).tolist()\n",
    "for i, block in enumerate(model.backbone.layers):\n",
    "    # Assuming the transformer blocks are in backbone.layers\n",
    "    model.backbone.layers[i].r = r_values[i]\n",
    "\n",
    "\n",
    "pixel_wise_acc_list = []\n",
    "\n",
    "prunes = []\n",
    "\n",
    "reduced_tokens_heatmap = None\n",
    "\n",
    "# Display the image\n",
    "for i, img_path in enumerate(VIDEO_GAIMERSHEIM):\n",
    "\n",
    "    print('image', i)\n",
    "    \n",
    "    result = inference_segmentor(model, img_path)\n",
    "    result_og = inference_segmentor(model_og, img_path)\n",
    "\n",
    "    result1 = result[0] - result_og[0]\n",
    "    binary_result = (result1 != 0).astype(int)\n",
    "    \n",
    "    percentage_ones = (binary_result.sum() / binary_result.size) * 100\n",
    "    pixel_wise_acc_list.append(percentage_ones)\n",
    "\n",
    "    reduction = []\n",
    "\n",
    "    for i, layer in enumerate(model.backbone.layers):\n",
    "\n",
    "        if layer.unm_idx is not None:\n",
    "            \n",
    "            if layer.src_idx is not None:\n",
    "                reduction.append(layer.src_idx.shape[1])\n",
    "            else:\n",
    "                reduction.append(0)\n",
    "\n",
    "    prunes.append(reduction)\n",
    "\n",
    "    pruned_tokens = []\n",
    "    for i, layer in reversed(list(enumerate(model.backbone.layers))):\n",
    "        if layer.unm_idx is not None:\n",
    "            if len(pruned_tokens) == 0:\n",
    "                pruned_tokens = [0] * len(layer.unm_idx[0])\n",
    "\n",
    "            if layer.src_idx is not None:\n",
    "                temp = pruned_tokens\n",
    "\n",
    "                pruned_tokens = [0] * ( len(layer.unm_idx[0]) + len(layer.src_idx[0]))\n",
    "\n",
    "                for i, unm_idx in enumerate(layer.unm_idx[0]):\n",
    "                    pruned_tokens[unm_idx] = temp[i]\n",
    "\n",
    "                for i, src_idx in enumerate(layer.src_idx[0]):\n",
    "                    pruned_tokens[src_idx] = 1\n",
    "\n",
    "    if len(pruned_tokens) == 0:\n",
    "        pruned_tokens = [0]*1024\n",
    "        \n",
    "    tokens_reduced = np.array(pruned_tokens).reshape((32, 32))\n",
    "    \n",
    "    # scaled_tokens_reduced = np.repeat(np.repeat(tokens_reduced, 16, axis=0), 16, axis=1)\n",
    "    zoom_factor = 1208 / 32\n",
    "    scaled_tokens_reduced = zoom(tokens_reduced, zoom=(zoom_factor, zoom_factor), order=1)\n",
    "\n",
    "    if reduced_tokens_heatmap is not None:\n",
    "        reduced_tokens_heatmap += scaled_tokens_reduced\n",
    "    else:\n",
    "        reduced_tokens_heatmap = scaled_tokens_reduced\n",
    "\n",
    "# prunes\n",
    "np.save(f\"benchmarking/tokens_pruned_lin_0.995_0.93.npy\", prunes)\n",
    "\n",
    "# pixel_wise_acc_list\n",
    "np.save(f\"benchmarking/pixel_wise_acc_list_lin_0.995_0.93.npy\", pixel_wise_acc_list)\n",
    "\n",
    "# encode times\n",
    "encode_times_np = np.array(model.encode_times)\n",
    "np.save(f\"benchmarking/encode_times_lin_0.995_0.93.npy\", encode_times_np)\n",
    "\n",
    "# heatmap\n",
    "np.save(f\"benchmarking/reduced_tokens_heatmap_lin_0.995_0.93.npy\", reduced_tokens_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6239e2-42be-4ae3-8575-97ebd428fb20",
   "metadata": {},
   "source": [
    "## amount of reduction layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d267a2-5393-4113-b13a-7f1c4d5b596d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_file = '../configs/token_reducing_config_gpu.py'\n",
    "checkpoint_file = '../model/checkpoint/COCOstuff_49.9.pth'\n",
    "\n",
    "# r_values = np.arange(1, 0.60, -0.02).round(2).tolist()\n",
    "\n",
    "# print(r_values)\n",
    "\n",
    "deviders_of_24 = [24, 12, 8, 6, 4, 3, 2, 1]\n",
    "\n",
    "for reduction_interval in deviders_of_24:\n",
    "    # print(r_value)\n",
    "\n",
    "    # model\n",
    "    model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "    # Overwriting the r values\n",
    "    r_values = np.linspace(0.995, 0.93, len(model.backbone.layers)).tolist()\n",
    "    for i, block in enumerate(model.backbone.layers):\n",
    "        model.backbone.layers[i].r = r_values[i]\n",
    "        model.backbone.layers[i].reduction_interval = reduction_interval\n",
    "\n",
    "        # print(model.backbone.layers[i].reduction_interval)\n",
    "\n",
    "    pixel_wise_acc_list = []\n",
    "    \n",
    "    prunes = []\n",
    "\n",
    "    reduced_tokens_heatmap = None\n",
    "    \n",
    "    # Display the image\n",
    "    for i, img_path in enumerate(VIDEO_GAIMERSHEIM):\n",
    "    \n",
    "        print('image', i)\n",
    "        \n",
    "        result = inference_segmentor(model, img_path)\n",
    "        result_og = inference_segmentor(model_og, img_path)\n",
    "    \n",
    "        result1 = result[0] - result_og[0]\n",
    "        binary_result = (result1 != 0).astype(int)\n",
    "        \n",
    "        percentage_ones = (binary_result.sum() / binary_result.size) * 100\n",
    "        pixel_wise_acc_list.append(percentage_ones)\n",
    "    \n",
    "        reduction = []\n",
    "    \n",
    "        for i, layer in enumerate(model.backbone.layers):\n",
    "    \n",
    "            if layer.unm_idx is not None:\n",
    "                \n",
    "                if layer.src_idx is not None:\n",
    "                    reduction.append(layer.src_idx.shape[1])\n",
    "                else:\n",
    "                    reduction.append(0)\n",
    "    \n",
    "        prunes.append(reduction)\n",
    "\n",
    "        pruned_tokens = []\n",
    "        for i, layer in reversed(list(enumerate(model.backbone.layers))):\n",
    "            if layer.unm_idx is not None:\n",
    "                if len(pruned_tokens) == 0:\n",
    "                    pruned_tokens = [0] * len(layer.unm_idx[0])\n",
    "    \n",
    "                if layer.src_idx is not None:\n",
    "                    temp = pruned_tokens\n",
    "    \n",
    "                    pruned_tokens = [0] * ( len(layer.unm_idx[0]) + len(layer.src_idx[0]))\n",
    "    \n",
    "                    for i, unm_idx in enumerate(layer.unm_idx[0]):\n",
    "                        pruned_tokens[unm_idx] = temp[i]\n",
    "    \n",
    "                    for i, src_idx in enumerate(layer.src_idx[0]):\n",
    "                        pruned_tokens[src_idx] = 1\n",
    "    \n",
    "        if len(pruned_tokens) == 0:\n",
    "            pruned_tokens = [0]*1024\n",
    "            \n",
    "        tokens_reduced = np.array(pruned_tokens).reshape((32, 32))\n",
    "        \n",
    "        # scaled_tokens_reduced = np.repeat(np.repeat(tokens_reduced, 16, axis=0), 16, axis=1)\n",
    "        zoom_factor = 1208 / 32\n",
    "        scaled_tokens_reduced = zoom(tokens_reduced, zoom=(zoom_factor, zoom_factor), order=1)\n",
    "\n",
    "        if reduced_tokens_heatmap is not None:\n",
    "            reduced_tokens_heatmap += scaled_tokens_reduced\n",
    "        else:\n",
    "            reduced_tokens_heatmap = scaled_tokens_reduced\n",
    "\n",
    "    # prunes\n",
    "    np.save(f\"benchmarking/tokens_pruned_lin_int_{reduction_interval}.npy\", prunes)\n",
    "    \n",
    "    # pixel_wise_acc_list\n",
    "    np.save(f\"benchmarking/pixel_wise_acc_list_lin_int_{reduction_interval}.npy\", pixel_wise_acc_list)\n",
    "\n",
    "    # encode times\n",
    "    encode_times_np = np.array(model.encode_times)\n",
    "    np.save(f\"benchmarking/encode_times_lin_int_{reduction_interval}.npy\", encode_times_np)\n",
    "\n",
    "    # heatmap\n",
    "    np.save(f\"benchmarking/reduced_tokens_heatmap_lin_int_{reduction_interval}.npy\", reduced_tokens_heatmap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dd4e2-56c8-411e-beb9-4bae162b70ce",
   "metadata": {},
   "source": [
    "## OG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebf885-e443-417b-910d-85e5b23f6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "config_file = '/project_ghent/woverbie/code/SegVit/configs/segvit/segvit_vit-l_jax_512x512_80k_cocostuff10k.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "model_og = init_segmentor(config_file, checkpoint_file, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0e2d3-99df-448d-95d8-e46a1468981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "square_imgs = sorted(glob('../dataset/squares/*.png'))\n",
    "\n",
    "for img in tqdm(square_imgs):\n",
    "    result = inference_segmentor(model_og, img)\n",
    "\n",
    "\n",
    "np.save(f\"benchmarking/model_og_encode_times_gpu.npy\", model_og.encode_times)\n",
    "np.save(f\"benchmarking/model_og_decode_times_gpu.npy\", model_og.decode_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e8a73-4de8-4913-b1aa-9948edecc322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flops\n",
    "import cv2\n",
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from mmseg.datasets.pipelines import Compose\n",
    "\n",
    "cfg = model_og.cfg  # The config file used to initialize the model\n",
    "test_pipeline = Compose(cfg.data.test.pipeline)\n",
    "\n",
    "def profile_inference(model, img):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], \n",
    "                 with_flops=True,  # Enable FLOP counting\n",
    "                 record_shapes=True) as prof:\n",
    "        # Perform inference\n",
    "        _ = inference_segmentor(model_og, img)\n",
    "\n",
    "    return prof\n",
    "\n",
    "flop_list = []\n",
    "\n",
    "# Display the image\n",
    "\n",
    "square_imgs = sorted(glob('../dataset/squares/*.png'))\n",
    "\n",
    "for i, img_path in tqdm(enumerate(square_imgs)):\n",
    "\n",
    "    prof = profile_inference(model_og, img_path)\n",
    "    \n",
    "    avg = sum(op.flops for op in prof.key_averages() if hasattr(op, 'flops'))\n",
    "\n",
    "    flop_list.append(avg)\n",
    "\n",
    "\n",
    "print(np.median(flop_list))\n",
    "print(np.mean(flop_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f502e9e-0086-4918-875b-0084de516f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(flop_list))\n",
    "print(np.mean(flop_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27218b-89e4-49b3-a49e-8f31e1d7edf3",
   "metadata": {},
   "source": [
    "# CPU TIMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cc66a-697a-49fa-b9e0-4f6ffefa8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "config_file_cpu = '/project_ghent/woverbie/code/configs/config.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "\n",
    "model_cpu = init_segmentor(config_file_cpu, checkpoint_file, device='cpu')\n",
    "\n",
    "# Overwriting the r values\n",
    "r_values = np.linspace(0.995, 0.93, len(model_cpu.backbone.layers)).tolist()\n",
    "for i, block in enumerate(model_cpu.backbone.layers):\n",
    "    model_cpu.backbone.layers[i].r = r_values[i]\n",
    "    model_cpu.backbone.layers[i].reduction_interval = 8\n",
    "\n",
    "\n",
    "\n",
    "test_set = VIDEO_GAIMERSHEIM[1000:2000]\n",
    "\n",
    "# Display the image\n",
    "for i, img_path in tqdm(enumerate(test_set)):\n",
    "    result = inference_segmentor(model_cpu, img_path)\n",
    "\n",
    "\n",
    "encode_times_cpu = np.array(model_cpu.encode_times)\n",
    "np.save(f\"benchmarking/final_token_reducing_cpu_encode_times.npy\", encode_times_cpu)\n",
    "\n",
    "decode_times_cpu = np.array(model_cpu.decode_times)\n",
    "np.save(f\"benchmarking/final_token_reducing_cpu_decode_times.npy\", decode_times_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7beb1-d77f-45f8-9fc7-817510ffe23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_og = '/project_ghent/woverbie/code/SegVit/configs/segvit/segvit_vit-l_jax_512x512_80k_cocostuff10k.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "model_og_cpu = init_segmentor(config_file_og, checkpoint_file, device='cpu')\n",
    "\n",
    "test_set = VIDEO_GAIMERSHEIM[1000:2000]\n",
    "\n",
    "# Display the image\n",
    "for i, img_path in tqdm(enumerate(test_set)):\n",
    "    result = inference_segmentor(model_og_cpu, img_path)\n",
    "\n",
    "encode_times_og_cpu = np.array(model_og_cpu.encode_times)\n",
    "np.save(f\"benchmarking/og_cpu_encode_times.npy\", encode_times_og_cpu)\n",
    "\n",
    "decode_times_og_cpu = np.array(model_og_cpu.decode_times)\n",
    "np.save(f\"benchmarking/og_cpu_decode_times.npy\", decode_times_og_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df3f91-1fa4-4086-89a0-b672f984e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('benchmarking/og_cpu_encode_times.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2435a30-6d14-4922-8e99-48a65a9dc7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31451d23-17ba-4eac-923d-a6a03f0abb0b",
   "metadata": {},
   "source": [
    "# FLOPS FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525541e-11c0-451a-b5f9-4192140c0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '/project_ghent/woverbie/code/configs/token_reducing_config_gpu.py'\n",
    "checkpoint_file = '../../benchmarking/SegVit_test_2/models/COCOstuff_49.9.pth'\n",
    "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "# Setting the r values\n",
    "\n",
    "r_values = np.linspace(0.995, 0.93, len(model.backbone.layers)).tolist()\n",
    "for i, block in enumerate(model.backbone.layers):\n",
    "    # Assuming the transformer blocks are in backbone.layers\n",
    "    model.backbone.layers[i].r = r_values[i]\n",
    "    model.backbone.layers[i].reduction_interval = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093d3a3-f681-4d0a-b295-b9756f422640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Test with an inference function\n",
    "import cv2\n",
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from mmseg.datasets.pipelines import Compose\n",
    "from tqdm import tqdm\n",
    "\n",
    "cfg = model.cfg  # The config file used to initialize the model\n",
    "test_pipeline = Compose(cfg.data.test.pipeline)\n",
    "\n",
    "prof_list = []\n",
    "\n",
    "def profile_inference(model, img):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], \n",
    "                 with_flops=True,  # Enable FLOP counting\n",
    "                 record_shapes=True) as prof:\n",
    "        # Perform inference\n",
    "        _ = inference_segmentor(model, img)\n",
    "        \n",
    "    return prof\n",
    "\n",
    "\n",
    "\n",
    "# Display the image\n",
    "for i, img_path in tqdm(enumerate(VIDEO_GAIMERSHEIM)):\n",
    "\n",
    "    prof = profile_inference(model, img_path)\n",
    "    \n",
    "    avg = sum(op.flops for op in prof.key_averages() if hasattr(op, 'flops'))\n",
    "\n",
    "    print(avg)\n",
    "\n",
    "    prof_list.append(avg)\n",
    "\n",
    "\n",
    "\n",
    "np.save(f\"benchmarking/final_model_flops.npy\", prof_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4833ad-a48b-4d24-a649-4bad4ef43819",
   "metadata": {},
   "source": [
    "# VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fabfd4-da3e-4a38-8b0b-1c00f6021425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(img_array, crop_size):\n",
    "    \"\"\"\n",
    "    Crop the center of the image.\n",
    "    \n",
    "    Parameters:\n",
    "        img_array (numpy.ndarray): Input image as a NumPy array.\n",
    "        crop_size (tuple): Desired crop size (height, width).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Center-cropped image.\n",
    "    \"\"\"\n",
    "    h, w, *channels = img_array.shape\n",
    "    crop_height, crop_width = crop_size\n",
    "\n",
    "    # Calculate the coordinates for the crop\n",
    "    start_y = (h - crop_height) // 2\n",
    "    start_x = (w - crop_width) // 2\n",
    "\n",
    "    # Perform the crop\n",
    "    cropped_img = img_array[start_y:start_y + crop_height, start_x:start_x + crop_width]\n",
    "\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36facf8a-afc5-416e-8d28-fd63843c7743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff9191-36d6-4a67-a08c-6f654741b882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "config_file = '../configs/token_reducing_config_gpu.py'\n",
    "checkpoint_file = '../model/checkpoint/COCOstuff_49.9.pth'\n",
    "\n",
    "model = init_segmentor(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "r_values = np.linspace(0.995, 0.93, len(model.backbone.layers)).tolist()\n",
    "for i, block in enumerate(model.backbone.layers):\n",
    "    # Assuming the transformer blocks are in backbone.layers\n",
    "    model.backbone.layers[i].r = r_values[i]\n",
    "\n",
    "\n",
    "pixel_wise_acc_list = [0] * 150\n",
    "\n",
    "prunes = [[0,0,0]] * 150\n",
    "\n",
    "flop_list = [0] * 150\n",
    "\n",
    "# Load the NumPy array\n",
    "flop_array = np.load('/project_ghent/woverbie/code/notebooks/benchmarking/final_model_flops.npy')\n",
    "\n",
    "# Concatenate the list and NumPy array\n",
    "combined = np.concatenate((np.array(flop_list), flop_array))\n",
    "\n",
    "reduced_tokens_heatmap = None\n",
    "\n",
    "# Display the image\n",
    "for image_id_, img_path in tqdm(enumerate(VIDEO_GAIMERSHEIM)):\n",
    "\n",
    "    # print('image', image_id_)\n",
    "    \n",
    "    result = inference_segmentor(model, img_path)\n",
    "    result_og = inference_segmentor(model_og, img_path)\n",
    "\n",
    "    result1 = result[0] - result_og[0]\n",
    "    binary_result = (result1 != 0).astype(int)\n",
    "    \n",
    "    percentage_ones = (binary_result.sum() / binary_result.size) * 100\n",
    "    pixel_wise_acc_list.append(percentage_ones)\n",
    "\n",
    "    reduction = []\n",
    "\n",
    "    for i, layer in enumerate(model.backbone.layers):\n",
    "\n",
    "        if layer.unm_idx is not None:\n",
    "            \n",
    "            if layer.src_idx is not None:\n",
    "                reduction.append(layer.src_idx.shape[1])\n",
    "            else:\n",
    "                reduction.append(0)\n",
    "\n",
    "    prunes.append(reduction)\n",
    "\n",
    "    pruned_tokens = []\n",
    "    for i, layer in reversed(list(enumerate(model.backbone.layers))):\n",
    "        if layer.unm_idx is not None:\n",
    "            if len(pruned_tokens) == 0:\n",
    "                pruned_tokens = [0] * len(layer.unm_idx[0])\n",
    "\n",
    "            if layer.src_idx is not None:\n",
    "                temp = pruned_tokens\n",
    "\n",
    "                pruned_tokens = [0] * ( len(layer.unm_idx[0]) + len(layer.src_idx[0]))\n",
    "\n",
    "                for i, unm_idx in enumerate(layer.unm_idx[0]):\n",
    "                    pruned_tokens[unm_idx] = temp[i]\n",
    "\n",
    "                for i, src_idx in enumerate(layer.src_idx[0]):\n",
    "                    pruned_tokens[src_idx] = 1\n",
    "\n",
    "    if len(pruned_tokens) == 0:\n",
    "        pruned_tokens = [0]*1024\n",
    "        \n",
    "    tokens_reduced = np.array(pruned_tokens).reshape((32, 32))\n",
    "    \n",
    "    # scaled_tokens_reduced = np.repeat(np.repeat(tokens_reduced, 16, axis=0), 16, axis=1)\n",
    "    zoom_factor = 1208 / 32\n",
    "    scaled_tokens_reduced = zoom(tokens_reduced, zoom=(zoom_factor, zoom_factor), order=1)\n",
    "\n",
    "    if reduced_tokens_heatmap is not None:\n",
    "        reduced_tokens_heatmap += scaled_tokens_reduced\n",
    "    else:\n",
    "        reduced_tokens_heatmap = scaled_tokens_reduced\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "    image = Image.open(img_path)\n",
    "    image_array = np.array(image)\n",
    "    cropped_image = center_crop(image_array, (1208, 1208))\n",
    "    axs[0, 0].imshow(cropped_image)\n",
    "    axs[0, 0].axis('off')\n",
    "    axs[0, 0].set_title('Input Image')\n",
    "    \n",
    "    axs[0, 1].imshow(result[0])\n",
    "    axs[0, 1].axis('off')\n",
    "    axs[0, 1].set_title('Result ToRe')\n",
    "\n",
    "    axs[0, 2].imshow(result_og[0])\n",
    "    axs[0, 2].axis('off')\n",
    "    axs[0, 2].set_title('Result Original')\n",
    "    \n",
    "    tokens_pruned_sum = np.sum(prunes, axis=1)\n",
    "    axs[1, 0].plot(tokens_pruned_sum[-150:], label='')\n",
    "    axs[1, 0].set_title('Tokens Pruned')\n",
    "    axs[1, 0].set_ylim(0, 1024)\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid()\n",
    "\n",
    "    acc = [100 - item for item in pixel_wise_acc_list]\n",
    "    axs[1, 1].plot(acc[-150:], label='')\n",
    "    axs[1, 1].set_title('Pixel-Wise Accuracy')\n",
    "    axs[1, 1].set_ylim(0, 100)\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid()\n",
    "    \n",
    "    axs[1, 3].imshow(scaled_tokens_reduced)\n",
    "    axs[1, 3].axis('off')\n",
    "    axs[1, 3].set_title('Tokens Reduced')\n",
    "    \n",
    "    axs[0, 3].imshow(binary_result)\n",
    "    axs[0, 3].axis('off')\n",
    "    axs[0, 3].set_title('Diff ToRe and Original')\n",
    "\n",
    "\n",
    "    axs[1, 2].plot(combined[image_id_:image_id_+150], label='')\n",
    "    axs[1, 2].set_title('FLOPs')\n",
    "    axs[1, 2].set_ylim(0, 800000000000)\n",
    "    axs[1, 2].grid()\n",
    "    \n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"video/visualisation_{image_id_}.png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5298de-2e60-41a5-8a35-54de67e917fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "output_file = \"output_video.mp4\"\n",
    "frame_rate = 30  # Frames per second\n",
    "\n",
    "# Use glob to get all PNG files in the folder\n",
    "image_files = glob(\"video/*.png\")\n",
    "image_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # Sort by numeric order in filename\n",
    "\n",
    "\n",
    "# Read the first image to get the dimensions\n",
    "first_image = cv2.imread(image_files[0])\n",
    "height, width, _ = first_image.shape\n",
    "\n",
    "# Define the video codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "video_writer = cv2.VideoWriter(output_file, fourcc, frame_rate, (width, height))\n",
    "\n",
    "# Loop through the sorted images and write them to the video\n",
    "for image_path in image_files:\n",
    "    frame = cv2.imread(image_path)\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release the VideoWriter object\n",
    "video_writer.release()\n",
    "print(f\"Video saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa715952-fac0-486f-b764-4fcc6691b128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
